{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8c5326f",
   "metadata": {},
   "source": [
    "BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d1139c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\abhay\n",
      "[nltk_data]     singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce03bae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "para=\"\"\"He is eating veg. she is eating nonveg. both are eating food\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6ccd593",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents=nltk.sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "064d9785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He is eating veg.', 'she is eating nonveg.', 'both are eating food']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10715073",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=nltk.word_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70494c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'is',\n",
       " 'eating',\n",
       " 'veg',\n",
       " '.',\n",
       " 'she',\n",
       " 'is',\n",
       " 'eating',\n",
       " 'nonveg',\n",
       " '.',\n",
       " 'both',\n",
       " 'are',\n",
       " 'eating',\n",
       " 'food']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cdaf96",
   "metadata": {},
   "source": [
    "BoW nltk re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29e1782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3 code for preprocessing text\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# execute the text here as :\n",
    "# text = \"\"\" # place text here \"\"\"\n",
    "text=para\n",
    "corpus=[]\n",
    "dataset = nltk.sent_tokenize(text)\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].lower()\n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i])\n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "137492f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he is eating veg ', 'she is eating nonveg ', 'both are eating food']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa17e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cd7986d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\abhay\n",
      "[nltk_data]     singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f009514",
   "metadata": {},
   "outputs": [],
   "source": [
    "port=PorterStemmer()\n",
    "lemm=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a50ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean=[port.stem(word) for word in dataset if word not in set (stopwords.words('english'))]\n",
    "clean=' '.join(clean)\n",
    "corpus.append(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07bc2f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he is eating veg  she is eating nonveg  both are eating food',\n",
       " 'he is eating veg  she is eating nonveg  both are eating food']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4dab3041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he is eating veg ', 'she is eating nonveg ', 'both are eating food']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a32a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "word2count = {}\n",
    "for data in dataset:\n",
    "\twords = nltk.word_tokenize(data)\n",
    "\tfor word in words:\n",
    "\t\tif word not in word2count.keys():\n",
    "\t\t\tword2count[word] = 1\n",
    "\t\telse:\n",
    "\t\t\tword2count[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14fe7969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'he': 1,\n",
       " 'is': 2,\n",
       " 'eating': 3,\n",
       " 'veg': 1,\n",
       " 'she': 1,\n",
       " 'nonveg': 1,\n",
       " 'both': 1,\n",
       " 'are': 1,\n",
       " 'food': 1}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "38385e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "freq_words = heapq.nlargest(100, word2count, key=word2count.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66eb5ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for data in dataset:\n",
    "\tvector = []\n",
    "\tfor word in freq_words:\n",
    "\t\tif word in nltk.word_tokenize(data):\n",
    "\t\t\tvector.append(1)\n",
    "\t\telse:\n",
    "\t\t\tvector.append(0)\n",
    "\tX.append(vector)\n",
    "X = np.asarray(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07f16195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 1, 1]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ad8ba6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'food'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df294b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232dc8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0ddb8e2",
   "metadata": {},
   "source": [
    "tf idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ad43d3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "idf values:\n",
      "eat : 1.0\n",
      "food : 1.6931471805599454\n",
      "nonveg : 1.6931471805599454\n",
      "veg : 1.6931471805599454\n",
      "\n",
      "Word indexes:\n",
      "{'eat': 0, 'veg': 3, 'nonveg': 2, 'food': 1}\n",
      "\n",
      "tf-idf value:\n",
      "  (0, 3)\t0.8610369959439764\n",
      "  (0, 0)\t0.5085423203783267\n",
      "  (1, 2)\t0.8610369959439764\n",
      "  (1, 0)\t0.5085423203783267\n",
      "  (2, 1)\t0.8610369959439764\n",
      "  (2, 0)\t0.5085423203783267\n",
      "\n",
      "tf-idf values in matrix form:\n",
      "[[0.50854232 0.         0.         0.861037  ]\n",
      " [0.50854232 0.         0.861037   0.        ]\n",
      " [0.50854232 0.861037   0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# import required module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# assign documents\n",
    "d0 = 'eat veg'\n",
    "d1 = 'eat nonveg'\n",
    "d2 = 'eat food'\n",
    "\n",
    "# merge documents into a single corpus\n",
    "string = [d0, d1, d2]\n",
    "\n",
    "# create object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# get tf-df values\n",
    "result = tfidf.fit_transform(string)\n",
    "\n",
    "# get idf values\n",
    "print('\\nidf values:')\n",
    "for ele1, ele2 in zip(tfidf.get_feature_names(), tfidf.idf_):\n",
    "\tprint(ele1, ':', ele2)\n",
    "\n",
    "# get indexing\n",
    "print('\\nWord indexes:')\n",
    "print(tfidf.vocabulary_)\n",
    "\n",
    "# display tf-idf values\n",
    "print('\\ntf-idf value:')\n",
    "print(result)\n",
    "\n",
    "# in matrix form\n",
    "print('\\ntf-idf values in matrix form:')\n",
    "print(result.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cbffe53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eat veg', 'eat nonveg', 'eat food']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "49d9bc7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2d888857",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_val=tfidf.fit_transform(string).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c1c029e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50854232, 0.        , 0.        , 0.861037  ],\n",
       "       [0.50854232, 0.        , 0.861037  , 0.        ],\n",
       "       [0.50854232, 0.861037  , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ffd20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "235c1ac1",
   "metadata": {},
   "source": [
    "wordnet examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9aed7b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset name :  bank.n.01\n",
      "\n",
      "Synset meaning :  sloping land (especially the slope beside a body of water)\n",
      "\n",
      "Synset example :  ['they pulled the canoe up on the bank', 'he sat on the bank of the river and watched the currents']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\abhay\n",
      "[nltk_data]     singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "syn = wordnet.synsets('bank')[0]\n",
    "\n",
    "print (\"Synset name : \", syn.name())\n",
    "\n",
    "# Defining the word\n",
    "print (\"\\nSynset meaning : \", syn.definition())\n",
    "\n",
    "# list of phrases that use the word in context\n",
    "print (\"\\nSynset example : \", syn.examples())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2123a3ea",
   "metadata": {},
   "source": [
    "Hypernyms and Hyponyms – \n",
    "\n",
    "Hypernyms: More abstract terms \n",
    "Hyponyms: More specific terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3fe3dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset name :  bank.n.01\n",
      "\n",
      "Synset abstract term :  [Synset('slope.n.01')]\n",
      "\n",
      "Synset specific term :  [Synset('ascent.n.01'), Synset('bank.n.01'), Synset('bank.n.07'), Synset('canyonside.n.01'), Synset('coast.n.02'), Synset('descent.n.05'), Synset('escarpment.n.01'), Synset('hillside.n.01'), Synset('mountainside.n.01'), Synset('piedmont.n.02'), Synset('ski_slope.n.01')]\n",
      "\n",
      "Synset root hypernerm :  [Synset('entity.n.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets('bank')[0]\n",
    "\n",
    "print (\"Synset name : \", syn.name())\n",
    "\n",
    "print (\"\\nSynset abstract term : \", syn.hypernyms())\n",
    "\n",
    "print (\"\\nSynset specific term : \",\n",
    "\tsyn.hypernyms()[0].hyponyms())\n",
    "\n",
    "syn.root_hypernyms()\n",
    "\n",
    "print (\"\\nSynset root hypernerm : \", syn.root_hypernyms())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2b3aeffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syn tag :  n\n",
      "Syn tag :  v\n",
      "Syn tag :  a\n",
      "Syn tag :  r\n"
     ]
    }
   ],
   "source": [
    "syn = wordnet.synsets('hello')[0]\n",
    "print (\"Syn tag : \", syn.pos())\n",
    "\n",
    "syn = wordnet.synsets('doing')[0]\n",
    "print (\"Syn tag : \", syn.pos())\n",
    "\n",
    "syn = wordnet.synsets('beautiful')[0]\n",
    "print (\"Syn tag : \", syn.pos())\n",
    "\n",
    "syn = wordnet.synsets('quickly')[0]\n",
    "print (\"Syn tag : \", syn.pos())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac980ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
